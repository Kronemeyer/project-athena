{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minerva - Task 1 Report\n",
    "===\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract\n",
    "\n",
    "In Task One, we generated adversarial examples (AE) in the context of the zero-knowledge threat models. Using the Carlini - Wagner (CW), the Basic Iterative Method (BIM), and the Projected Gradient Descent (PGD) attacks, we crafted 18,000 adversarial examples that were derived from 18 attacks on 1000 adversarial examples. Our experiment has shown that a PGD attack is the most effective in deceiving an undefended model (UM). While PGD used miniscule epsilon values, a CW attack is most effective against an UM with a constant epsilon causing the error rate to increase gradually and a BIM attack is the most effective against an UM as the epsilon values increase. We have concluded that while BIM and CW attacks fool the UM with a 100% success rate, the PGD attack exhibited the highest rate of error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach\n",
    "\n",
    "For this task, we decided to use increasing strength variants of the Carlini - Wagner attack (CW), the Basic Iterative Method (BIM) attack, and the Projected Gradient Descent (PGD) attack. In total, 18 attacks were used on a sample size of 1000 adversarial examples (AE), effectively creating 18,000 adversarial examples. These AE's were then tested for robustness on an undefended model, the Athena Framework, and the PGD mode.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimental Settings  \n",
    "\n",
    "#### CW Attacks\n",
    "The CW Attacks all used the Linf configuration. This is our first experience with these types of attacks, therefore, we have decided to hold one variable as a constant while gradually increasing the strength of the other variable.  \n",
    "\n",
    "The specific values used:\n",
    "* epsilon constant: .10, lrs: .2, .4, .6, .8\n",
    "* lr constant: .10, epsilons: .2, .4, .6, .8\n",
    "\n",
    "#### BIM Attacks\n",
    "The BIM attacks are simply varied by their epsilon strength. Initially, it was found to be incrementing by 20%, so that the CW attacks and the BIM attacks easily fooled the undefended model with a 100% success rate. Therefore, lower epsilons were implemented to provide an increasing rate of effectiveness in deception, while anticipating a similar increase in adversarial robustness when the AE's were introduced into the Athena and PGD models.\n",
    "\n",
    "The specific values used:\n",
    "* epsilons: .01, .05, .10, .15, .20\n",
    "\n",
    "#### PGD Attacks\n",
    "Similar to the BIM attacks, the values of the epsilon chosen for the PGD attacks were also chosen at low incremental values. These attacks seemed to fool the undefended model much easier than the other two attacks and as such, the epsilons chosen were extremely small.\n",
    "\n",
    "The specific values used:\n",
    "* epsilons: .025, .05, .075, .10, .15\n",
    "\n",
    "### Creating the AE's\n",
    "In order to create the AE's the json \"attack-zk-mnist.json\" was created with the variables for all 18 attacks. This was then loaded into craft_adversarial_examples.py to create 1000 images per attack. The AE's were saved to output directory, /results, and the files were named per the descriptions relevant to that individual configuration.  \n",
    "\n",
    "model = ../configs/experiment/model-mnist.json  \n",
    "data = ../configs/experiment/data-mnist.json  \n",
    "labels also retrieved from the data-mnist.json  \n",
    "attack_configs = ../configs/experiment/attack-zk-mnist.json  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ae(model, data, labels, attack_configs, save=True, output_dir=\"../../results\"):\n",
    "    \"\"\"\n",
    "    Generate adversarial examples\n",
    "    :param model: WeakDefense. The targeted model.\n",
    "    :param data: array. The benign samples to generate adversarial for.\n",
    "    :param labels: array or list. The true labels.\n",
    "    :param attack_configs: dictionary. Attacks and corresponding settings.\n",
    "    :param save: boolean. True, if save the adversarial examples.\n",
    "    :param output_dir: str or path. Location to save the adversarial examples.\n",
    "        It cannot be None when save is True.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    img_rows, img_cols = data.shape[1], data.shape[2]\n",
    "    num_attacks = attack_configs.get(\"num_attacks\")\n",
    "    data_loader = (data, labels)\n",
    "\n",
    "    if len(labels.shape) > 1:\n",
    "        labels = np.array([np.argmax(p) for p in labels])\n",
    "        # might have to convert this to an array\n",
    "\n",
    "    # generate attacks one by one\n",
    "    for id in range(num_attacks):\n",
    "        key = \"configs{}\".format(id)\n",
    "        config = attack_configs.get(key)\n",
    "        data_adv = generate(model=model,\n",
    "                            data_loader=data_loader,\n",
    "                            attack_args=attack_configs.get(key)\n",
    "                            )\n",
    "\n",
    "        # predict the adversarial examples\n",
    "        predictions = model.predict(data_adv)\n",
    "        predictions = np.array([np.argmax(p) for p in predictions])\n",
    "\n",
    "        error_rate = metrics.error_rate_single(predictions, labels)\n",
    "        print(config.get('description') + ' Error Rate: ' + str(error_rate))\n",
    "\n",
    "\n",
    "        # # plotting some examples\n",
    "        num_plotting = min(data.shape[0], 3)\n",
    "        for i in range(num_plotting):\n",
    "            img = data_adv[i].reshape((img_rows, img_cols))\n",
    "            plt.imshow(img, cmap='gray')\n",
    "            title = '{}: {}->{}'.format(attack_configs.get(key).get(\"description\"),\n",
    "                                        labels[i],\n",
    "                                        predictions[i]\n",
    "                                        )\n",
    "            plt.title(title)\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "\n",
    "        # save the adversarial example\n",
    "        if save:\n",
    "            if output_dir is None:\n",
    "                raise ValueError(\"Cannot save images to a none path.\")\n",
    "            # save with a random name\n",
    "            file = os.path.join(output_dir, \"minerva_AE-{}.npy\".format(config.get('description')))\n",
    "            print(\"Save the adversarial examples to file [{}].\".format(file))\n",
    "            np.save(file, data_adv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the AE's against the Models\n",
    "Once the AE's had been created and saved, the AE file names were included in the data-mnist.json in order to evaluate the effectiveness of the attacks against three distinct models.\n",
    "\n",
    "1. Undefended Model  \n",
    "    - The undefended model is a blank model that has no adversarial robustness training. It can identify the numbers 0-9 effectively, given the pictures do not have an adversarial attack transformations.\n",
    "2. The Athena Framework  \n",
    "    - Our Athena framework consisted of the config1 - config20 weak defences integrated into an ensemble. There was no logical reasoning for using these defences, they were just chosen to have a decently large selection from the available weak defences.\n",
    "3. A PGT-ADT Trained Model  \n",
    "    - This was the baseline defence model used to compare against the robustness of the Undefended model and the Athena ensemble.\n",
    "    \n",
    "The robustness results were calculated using the model's guess compared to the true labels of the AE. These results were then output to a file for data collection and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(trans_configs, model_configs,\n",
    "             data_configs, save=True, output_dir=None):\n",
    "    \"\"\"\n",
    "    Apply transformation(s) on images.\n",
    "    :param trans_configs: dictionary. The collection of the parameterized transformations to test.\n",
    "        in the form of\n",
    "        { configsx: {\n",
    "            param: value,\n",
    "            }\n",
    "        }\n",
    "        The key of a configuration is 'configs'x, where 'x' is the id of corresponding weak defense.\n",
    "    :param model_configs:  dictionary. Defines model related information.\n",
    "        Such as, location, the undefended model, the file format, etc.\n",
    "    :param data_configs: dictionary. Defines data related information.\n",
    "        Such as, location, the file for the true labels, the file for the benign samples,\n",
    "        the files for the adversarial examples, etc.\n",
    "    :param save: boolean. Save the transformed sample or not.\n",
    "    :param output_dir: path or str. The location to store the transformed samples.\n",
    "        It cannot be None when save is True.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Load the baseline defense (PGD-ADT model)\n",
    "    baseline = load_lenet(file=model_configs.get('pgd_trained'), trans_configs=None,\n",
    "                                  use_logits=False, wrap=False)\n",
    "\n",
    "    # get the undefended model (UM)\n",
    "    file = os.path.join(model_configs.get('dir'), model_configs.get('um_file'))\n",
    "    undefended = load_lenet(file=file,\n",
    "                            trans_configs=trans_configs.get('configs0'),\n",
    "                            wrap=True)\n",
    "    print(\">>> um:\", type(undefended))\n",
    "\n",
    "    # load weak defenses into a pool\n",
    "    pool, _ = load_pool(trans_configs=trans_configs,\n",
    "                        model_configs=model_configs,\n",
    "                        active_list=True,\n",
    "                        wrap=True)\n",
    "    # create an AVEP ensemble from the WD pool\n",
    "    wds = list(pool.values())\n",
    "    print(\">>> wds:\", type(wds), type(wds[0]))\n",
    "    ensemble = Ensemble(classifiers=wds, strategy=ENSEMBLE_STRATEGY.AVEP.value)\n",
    "\n",
    "    # load the benign samples\n",
    "    bs_file = os.path.join(data_configs.get('dir'), data_configs.get('bs_file'))\n",
    "    x_bs = np.load(bs_file)\n",
    "    img_rows, img_cols = x_bs.shape[1], x_bs.shape[2]\n",
    "\n",
    "    # load the corresponding true labels, take just the first 1000\n",
    "    label_file = os.path.join(data_configs.get('dir'), data_configs.get('label_file'))\n",
    "    labels = np.load(label_file)\n",
    "    labels = labels[:1000]\n",
    "\n",
    "    # get indices of benign samples that are correctly classified by the targeted model\n",
    "    print(\">>> Evaluating UM on [{}], it may take a while...\".format(bs_file))\n",
    "    pred_bs = undefended.predict(x_bs)\n",
    "    corrections = get_corrections(y_pred=pred_bs, y_true=labels)\n",
    "\n",
    "    if save:\n",
    "        if output_dir is None:\n",
    "            raise ValueError(\"Cannot save to a none path.\")\n",
    "        # save with a random name\n",
    "        f = os.path.join(output_dir, \"minerva_AE-results.txt\")\n",
    "        out_file = open(f, 'w')\n",
    "\n",
    "    # Evaluate AEs.\n",
    "    ae_list = data_configs.get('ae_files')\n",
    "    for _ in range(len(ae_list)):\n",
    "        results = {}\n",
    "        ae_file = os.path.join(data_configs.get('dir'), ae_list[_])\n",
    "        print(ae_list[_])\n",
    "        print(ae_file)\n",
    "        x_adv = np.load(ae_file)\n",
    "\n",
    "        # evaluate the undefended model on the AE\n",
    "        print(\">>> Evaluating UM on [{}], it may take a while...\".format(ae_file))\n",
    "        pred_adv_um = undefended.predict(x_adv)\n",
    "        err_um = error_rate(y_pred=pred_adv_um, y_true=labels, correct_on_bs=corrections)\n",
    "        # track the result\n",
    "        results['UM'] = err_um\n",
    "\n",
    "        # evaluate the ensemble on the AE\n",
    "        print(\">>> Evaluating ensemble on [{}], it may take a while...\".format(ae_file))\n",
    "        pred_adv_ens = ensemble.predict(x_adv)\n",
    "        err_ens = error_rate(y_pred=pred_adv_ens, y_true=labels, correct_on_bs=corrections)\n",
    "        # track the result\n",
    "        results['Ensemble'] = err_ens\n",
    "\n",
    "        # evaluate the baseline on the AE\n",
    "        print(\">>> Evaluating baseline model on [{}], it may take a while...\".format(ae_file))\n",
    "        pred_adv_bl = baseline.predict(x_adv)\n",
    "        err_bl = error_rate(y_pred=pred_adv_bl, y_true=labels, correct_on_bs=corrections)\n",
    "        # track the result\n",
    "        results['PGD-ADT'] = err_bl\n",
    "\n",
    "        out_file.write(\">>> Evaluations on [{}]:\\n{}\\n\".format(ae_file, results))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "[comment]: <> (TODO-DS: Must finish descriptions for all the images)\n",
    "\n",
    "As previously stated, our approach for the CW attacks consists of making one of the variables remain constant while we gradually increase the value of the other variables. The graphs below illustrate the resulting differences using this method.\n",
    "\n",
    "\n",
    "![CW-2-Const-Eps](Img/CW-2-Const-Eps.png)\n",
    "<div style=\"text-align: center\">\n",
    "    <em>Figure 1.0  CW Attack effectiveness on an Ensemble vs. PGD with a constant epsilon.</em>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "![CW-2-Const-Lw](Img/CW-2-Const-Lw.png)\n",
    "<div style=\"text-align: center\">\n",
    "    <em>Figure 1.1  CW Attack effectiveness on an Ensemble vs. PGD with a constant Lw.</em>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "![CW-All-Const-Esp](Img/CW-All-Const-Eps.png)\n",
    "<div style=\"text-align: center\">\n",
    "    <em>Figure 1.2  CW Attack effectiveness on an UM vs. Ensemble vs PGD with a constant epsilon.</em>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "![CW-All-Const-Lw](Img/CW-All-Const-Lw.png)\n",
    "<div style=\"text-align: center\">\n",
    "    <em>Figure 1.3  CW Attack effectiveness on an UM vs. Ensemble vs PGD with a constant Lw.</em>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "The line graph below describes the effectiveness of the BIM attack on a Ensemble model verses PGD model. As the epsilon is increased, the error rate of the Ensemble increases faster than the error rate of the PGD model.\n",
    "\n",
    "![BIM-2](Img/BIM-2.png)\n",
    "<div style=\"text-align: center\">\n",
    "    <em>Figure 1.4  BIM Attack effectiveness on an Ensemble vs. PGD.</em>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "The line graph below illistruates the effectiveness of the BIM attack on an UM verses Ensemble model verses PGD model. As the epsilon is increased, the error rate of the UM increases faster than the error rate of the Ensemble and PGD models.\n",
    "\n",
    "![BIM-All](Img/BIM-All.png)\n",
    "<div style=\"text-align: center\">\n",
    "    <em>Figure 1.5  BIM Attack effectiveness on an UM vs. Ensemble vs. PGD.</em>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "![PGD-2](Img/PGD-2.png)\n",
    "<div style=\"text-align: center\">\n",
    "    <em>Figure 1.6  PGD Attack effectiveness on an Ensemble vs. PGD.</em>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "![PGD-All](Img/PGD-All.png)\n",
    "<div style=\"text-align: center\">\n",
    "    <em>Figure 1.7  PGD Attack effectiveness on an UM vs. Ensemble vs. PGD.</em>\n",
    "</div>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The raw data:\n",
    "\n",
    "\\>>> Evaluations on [../../data/minerva/minerva_AE-BIM-eps0.01.npy]:  \n",
    "{'UM': 0.010111223458038422, 'Ensemble': 0.003033367037411527, 'PGD-ADT': 0.008088978766430738}  \n",
    "\\>>> Evaluations on [../../data/minerva/minerva_AE-BIM-eps0.05.npy]:  \n",
    "{'UM': 0.22143579373104147, 'Ensemble': 0.004044489383215369, 'PGD-ADT': 0.015166835187057633}  \n",
    "\\>>> Evaluations on [../../data/minerva/minerva_AE-BIM-eps0.1.npy]:  \n",
    "{'UM': 0.9120323559150657, 'Ensemble': 0.023255813953488372, 'PGD-ADT': 0.032355915065722954}  \n",
    "\\>>> Evaluations on [../../data/minerva/minerva_AE-BIM-eps0.15.npy]:  \n",
    "{'UM': 0.9888776541961577, 'Ensemble': 0.06471183013144591, 'PGD-ADT': 0.05864509605662285}  \n",
    "\\>>> Evaluations on [../../data/minerva/minerva_AE-BIM-eps0.2.npy]:  \n",
    "{'UM': 0.9888776541961577, 'Ensemble': 0.1506572295247725, 'PGD-ADT': 0.1102123356926188}  \n",
    "\\>>> Evaluations on [../../data/minerva/minerva_AE-CW-lw0.1-eps0.2.npy]:  \n",
    "{'UM': 0.1263902932254803, 'Ensemble': 0.003033367037411527, 'PGD-ADT': 0.014155712841253791}  \n",
    "\\>>> Evaluations on [../../data/minerva/minerva_AE-CW-lw0.1-eps0.4.npy]:  \n",
    "{'UM': 0.13751263902932254, 'Ensemble': 0.003033367037411527, 'PGD-ADT': 0.014155712841253791}  \n",
    "\\>>> Evaluations on [../../data/minerva/minerva_AE-CW-lw0.1-eps0.6.npy]:  \n",
    "{'UM': 0.13751263902932254, 'Ensemble': 0.003033367037411527, 'PGD-ADT': 0.014155712841253791}  \n",
    "\\>>> Evaluations on [../../data/minerva/minerva_AE-CW-lw0.1-eps0.8.npy]:  \n",
    "{'UM': 0.1486349848331648, 'Ensemble': 0.003033367037411527, 'PGD-ADT': 0.01314459049544995}  \n",
    "\\>>> Evaluations on [../../data/minerva/minerva_AE-CW-lw0.2-eps0.1.npy]:  \n",
    "{'UM': 0.4580384226491405, 'Ensemble': 0.003033367037411527, 'PGD-ADT': 0.03538928210313448}  \n",
    "\\>>> Evaluations on [../../data/minerva/minerva_AE-CW-lw0.4-eps0.1.npy]:  \n",
    "{'UM': 0.8574317492416582, 'Ensemble': 0.004044489383215369, 'PGD-ADT': 0.07785642062689585}  \n",
    "\\>>> Evaluations on [../../data/minerva/minerva_AE-CW-lw0.6-eps0.1.npy]:  \n",
    "{'UM': 0.9595551061678463, 'Ensemble': 0.019211324570273004, 'PGD-ADT': 0.1243680485338726}  \n",
    "\\>>> Evaluations on [../../data/minerva/minerva_AE-CW-lw0.8-eps0.1.npy]:  \n",
    "{'UM': 0.9686552072800809, 'Ensemble': 0.033367037411526794, 'PGD-ADT': 0.16481294236602628}  \n",
    "\\>>> Evaluations on [../../data/minerva/minerva_AE-PGD-eps0.1.npy]:  \n",
    "{'UM': 0.6723963599595552, 'Ensemble': 0.017189079878665317, 'PGD-ADT': 0.028311425682507583}  \n",
    "\\>>> Evaluations on [../../data/minerva/minerva_AE-PGD-eps0.05.npy]:  \n",
    "{'UM': 0.1486349848331648, 'Ensemble': 0.004044489383215369, 'PGD-ADT': 0.014155712841253791}  \n",
    "\\>>> Evaluations on [../../data/minerva/minerva_AE-PGD-eps0.15.npy]:  \n",
    "{'UM': 0.9332659251769464, 'Ensemble': 0.04044489383215369, 'PGD-ADT': 0.04954499494438827}  \n",
    "\\>>> Evaluations on [../../data/minerva/minerva_AE-PGD-eps0.025.npy]:  \n",
    "{'UM': 0.033367037411526794, 'Ensemble': 0.004044489383215369, 'PGD-ADT': 0.011122345803842264}  \n",
    "\\>>> Evaluations on [../../data/minerva/minerva_AE-PGD-eps0.075.npy]:  \n",
    "{'UM': 0.3923154701718908, 'Ensemble': 0.007077856420626896, 'PGD-ADT': 0.022244691607684528}  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Citation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
